{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNUV5Uf6M5eqUm9pjsBIXFb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YELHtyUhtkKx","executionInfo":{"status":"ok","timestamp":1755452315183,"user_tz":-60,"elapsed":3203464,"user":{"displayName":"Nikunj Drolia","userId":"08035614581806700833"}},"outputId":"35789806-1cfe-43a3-cbd7-a19dda7abe5c"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1200/1200 [1:43:36<00:00,  5.18s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Evaluation Done\n","F1 Score : 0.3228\n","F1 Score : 0.3275\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from huggingface_hub import InferenceClient\n","import pandas as pd\n","from sklearn.metrics import f1_score\n","from tqdm import tqdm\n","import base64, os\n","\n","# 📂 Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 📄 Load and sample 1200 rows\n","df = pd.read_csv(\"/content/drive/MyDrive/vision_benchmark/metadata/wildlife_dataset.csv\")\n","sample_df = df.sample(1200, random_state=42).reset_index(drop=True)\n","\n","# 🧠 Model setup\n","client = InferenceClient(\"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n","\n","# 🏷️ All valid labels\n","known_labels = df[\"label\"].str.lower().unique().tolist()\n","\n","# 🧠 Base64 encoding\n","def encode_image_base64(image_path):\n","    if not os.path.exists(image_path):\n","        raise FileNotFoundError(f\"Image not found: {image_path}\")\n","    with open(image_path, \"rb\") as f:\n","        return \"data:image/jpeg;base64,\" + base64.b64encode(f.read()).decode(\"utf-8\")\n","\n","# 🧠 Extract predicted label from response\n","def extract_predicted_label(response, known_labels):\n","    response = str(response).lower()\n","    for label in known_labels:\n","        if label in response:\n","            return label\n","    return \"unknown\"\n","\n","# 🧪 Run inference\n","results = []\n","\n","for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n","    image_path = row[\"image_path\"]\n","    prompt = row[\"prompt\"]\n","    ground_truth = row[\"label\"].lower()\n","\n","    try:\n","        img_b64 = encode_image_base64(image_path)\n","\n","        messages = [{\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"image_url\", \"image_url\": {\"url\": img_b64}},\n","                {\"type\": \"text\", \"text\": prompt}\n","            ]\n","        }]\n","\n","        response = client.chat.completions.create(messages=messages, max_tokens=100)\n","        answer = response.choices[0].message.content.strip().lower()\n","        predicted_label = extract_predicted_label(answer, known_labels)\n","\n","    except Exception as e:\n","        import traceback\n","        traceback.print_exc()\n","        answer = f\"error: {e}\"\n","        predicted_label = \"unknown\"\n","\n","    results.append({\n","        \"image_path\": image_path,\n","        \"prompt\": prompt,\n","        \"ground_truth\": ground_truth,\n","        \"model_response\": answer,\n","        \"predicted_label\": predicted_label\n","    })\n","\n","# 💾 Save output\n","df_out = pd.DataFrame(results)\n","output_path = \"/content/drive/MyDrive/vision_benchmark/metadata/wildlife_preds_llama4_scout_sample1200.csv\"\n","df_out.to_csv(output_path, index=False)\n","\n","# 📊 Evaluation (includes 'unknown')\n","f1_macro = f1_score(df_out[\"ground_truth\"], df_out[\"predicted_label\"], average=\"macro\", zero_division=0)\n","f1_micro = f1_score(df_out[\"ground_truth\"], df_out[\"predicted_label\"], average=\"micro\", zero_division=0)\n","\n","print(\"\\n✅ Evaluation Done\")\n","print(f\"F1 Score : {f1_macro:.4f}\")\n","print(f\"F1 Score : {f1_micro:.4f}\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LE_mTYdmNuLE","executionInfo":{"status":"ok","timestamp":1755472467952,"user_tz":-60,"elapsed":22074,"user":{"displayName":"Nikunj Drolia","userId":"08035614581806700833"}},"outputId":"54edde52-b2f1-44c3-c34e-190d82d62c5b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import f1_score\n","from datetime import datetime\n","import os\n","\n","# 📁 Paths to already-saved prediction CSVs\n","model_paths = {\n","    \"Meta-Llama4-Scout\": \"/content/drive/MyDrive/vision_benchmark/metadata/wildlife_preds_llama4_scout_sample1200.csv\",\n","    \"Meta-Llama4-Maverick\": \"/content/drive/MyDrive/vision_benchmark/metadata/wildlife_preds_llama_maverick_sample1200.csv\"\n","    # Add more models here\n","}\n","\n","# 📊 Store metrics\n","results = []\n","\n","for model_name, path in model_paths.items():\n","    try:\n","        df = pd.read_csv(path)\n","        f1_macro = f1_score(df[\"ground_truth\"], df[\"predicted_label\"], average=\"macro\", zero_division=0)\n","        f1_micro = f1_score(df[\"ground_truth\"], df[\"predicted_label\"], average=\"micro\", zero_division=0)\n","        accuracy_score = (df[\"ground_truth\"] == df[\"predicted_label\"]).mean()\n","\n","        results.append({\n","            \"Model\": model_name,\n","            \"Dataset\": \"Wildlife\",\n","            \"Date\": datetime.today().strftime('%Y-%m-%d'),\n","            \"F1_Macro\": round(f1_macro, 4),\n","            \"F1_Micro\": round(f1_micro, 4),\n","            \"Accuracy\": round(accuracy_score,4)\n","        })\n","\n","        print(f\"✅ {model_name} - Macro F1: {f1_macro:.4f}, Micro F1: {f1_micro:.4f}, Accuracy: {accuracy_score}\")\n","\n","    except Exception as e:\n","        print(f\"❌ Failed for {model_name}: {e}\")\n","\n","# 💾 Save all metrics to summary CSV\n","summary_df = pd.DataFrame(results)\n","summary_path = \"/content/drive/MyDrive/vision_benchmark/metadata/wildlife_comparison_final.csv\"\n","summary_df.to_csv(summary_path, mode='a', header=not os.path.exists(summary_path), index=False)\n","print(f\"\\n📄 Summary saved to: {summary_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EK9qH5qrNqIb","executionInfo":{"status":"ok","timestamp":1755472482501,"user_tz":-60,"elapsed":9208,"user":{"displayName":"Nikunj Drolia","userId":"08035614581806700833"}},"outputId":"46fb907e-a7ab-4389-a2e4-3b25251e3db8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Meta-Llama4-Scout - Macro F1: 0.3228, Micro F1: 0.3275, Accuracy: 0.3275\n","✅ Meta-Llama4-Maverick - Macro F1: 0.3806, Micro F1: 0.3875, Accuracy: 0.3875\n","\n","📄 Summary saved to: /content/drive/MyDrive/vision_benchmark/metadata/wildlife_comparison_final.csv\n"]}]}]}