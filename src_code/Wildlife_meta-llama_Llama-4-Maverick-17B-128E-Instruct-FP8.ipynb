{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOwj9bHFpLradvqkzIak5xM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tymZGs2XuK4t","executionInfo":{"status":"ok","timestamp":1755450761078,"user_tz":-60,"elapsed":1667227,"user":{"displayName":"Nikunj Drolia","userId":"08035614581806700833"}},"outputId":"64bbceab-2a8c-4703-cc80-aad25705fdc3"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [59:19<00:00,  2.97s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Evaluation Done\n","F1 Score : 0.3806\n","F1 Score : 0.3875\n"]}],"source":["from huggingface_hub import InferenceClient\n","import pandas as pd\n","from sklearn.metrics import f1_score\n","from tqdm import tqdm\n","import base64, os\n","\n","# ðŸ“‚ Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ðŸ“„ Load and sample 1200 rows\n","df = pd.read_csv(\"/content/drive/MyDrive/vision_benchmark/metadata/wildlife_dataset.csv\")\n","sample_df = df.sample(1200, random_state=42).reset_index(drop=True)\n","\n","# ðŸ§  Model setup\n","client = InferenceClient(\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\")\n","\n","# ðŸ·ï¸ All valid labels\n","known_labels = df[\"label\"].str.lower().unique().tolist()\n","\n","# ðŸ§  Base64 encoding\n","def encode_image_base64(image_path):\n","    if not os.path.exists(image_path):\n","        raise FileNotFoundError(f\"Image not found: {image_path}\")\n","    with open(image_path, \"rb\") as f:\n","        return \"data:image/jpeg;base64,\" + base64.b64encode(f.read()).decode(\"utf-8\")\n","\n","# ðŸ§  Extract predicted label from response\n","def extract_predicted_label(response, known_labels):\n","    response = str(response).lower()\n","    for label in known_labels:\n","        if label in response:\n","            return label\n","    return \"unknown\"\n","\n","# ðŸ§ª Run inference\n","results = []\n","\n","for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n","    image_path = row[\"image_path\"]\n","    prompt = row[\"prompt\"]\n","    ground_truth = row[\"label\"].lower()\n","\n","    try:\n","        img_b64 = encode_image_base64(image_path)\n","\n","        messages = [{\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"image_url\", \"image_url\": {\"url\": img_b64}},\n","                {\"type\": \"text\", \"text\": prompt}\n","            ]\n","        }]\n","\n","        response = client.chat.completions.create(messages=messages, max_tokens=100)\n","        answer = response.choices[0].message.content.strip().lower()\n","        predicted_label = extract_predicted_label(answer, known_labels)\n","\n","    except Exception as e:\n","        import traceback\n","        traceback.print_exc()\n","        answer = f\"error: {e}\"\n","        predicted_label = \"unknown\"\n","\n","    results.append({\n","        \"image_path\": image_path,\n","        \"prompt\": prompt,\n","        \"ground_truth\": ground_truth,\n","        \"model_response\": answer,\n","        \"predicted_label\": predicted_label\n","    })\n","\n","# ðŸ’¾ Save output\n","df_out = pd.DataFrame(results)\n","output_path = \"/content/drive/MyDrive/vision_benchmark/metadata/wildlife_preds_llama_maverick_sample1200.csv\"\n","df_out.to_csv(output_path, index=False)\n","\n","# ðŸ“Š Evaluation (includes 'unknown')\n","f1_macro = f1_score(df_out[\"ground_truth\"], df_out[\"predicted_label\"], average=\"macro\", zero_division=0)\n","f1_micro = f1_score(df_out[\"ground_truth\"], df_out[\"predicted_label\"], average=\"micro\", zero_division=0)\n","\n","print(\"\\nâœ… Evaluation Done\")\n","print(f\"F1 Score : {f1_macro:.4f}\")\n","print(f\"F1 Score : {f1_micro:.4f}\")\n"]}]}